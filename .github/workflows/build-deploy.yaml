name: Build & Deploy to Minikube

on:
  push:
    branches: [ main, stage ]
  workflow_dispatch:

jobs:
  build-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    env:
      IMAGE_NAME: myapp
      REGISTRY: ghcr.io
      OWNER: ${{ github.repository_owner }}
      TAG: ${{ github.sha }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Start Minikube
        uses: medyagh/setup-minikube@v0.0.15
        with:
          driver: docker
          kubernetes-version: v1.30.0
          start-args: "--addons=ingress,metrics-server"

      - name: Set up Helm
        uses: azure/setup-helm@v4

      - name: Install kube-prometheus-stack (Prometheus + Grafana)
        shell: bash
        run: |
          set -euo pipefail
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          helm upgrade --install kps prometheus-community/kube-prometheus-stack \
            -n monitoring --create-namespace --wait
          kubectl get pods -n monitoring

      - name: Apply monitoring resources
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f k8s/monitoring/servicemonitor-myapp.yaml
          kubectl apply -f k8s/monitoring/prometheus-rule.yaml
          # optional dashboard; only apply if the file exists
          if [ -f k8s/monitoring/grafana-dashboard.yaml ]; then
            kubectl apply -f k8s/monitoring/grafana-dashboard.yaml
          fi

      - name: Verify Synthetic alert firing (auto-detect + retry)
        shell: bash
        run: |
          set -euo pipefail

          # 1) Wait for Prometheus to be ready (statefulset created by kube-prometheus-stack)
          PSS=$(kubectl -n monitoring get statefulset \
            -l app.kubernetes.io/instance=kps,app.kubernetes.io/name=kube-prometheus-stack-prometheus \
            -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          if [ -n "$PSS" ]; then
            kubectl -n monitoring rollout status statefulset/$PSS --timeout=300s || true
          fi

          # 2) Discover the Prometheus service (labels used by kube-prometheus-stack)
          SVC=$(kubectl -n monitoring get svc \
            -l app.kubernetes.io/instance=kps,app.kubernetes.io/name=kube-prometheus-stack-prometheus \
            -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)

          # 3) Fallback: try common service names
          if [ -z "${SVC:-}" ]; then
            for CAND in \
              kps-kube-prometheus-stack-prometheus \
              kube-prometheus-stack-prometheus \
              prometheus-operated
            do
              if kubectl -n monitoring get svc "$CAND" >/dev/null 2>&1; then
                SVC="$CAND"; break
              fi
            done
          fi

          if [ -z "${SVC:-}" ]; then
            echo "Could not find Prometheus service in 'monitoring' namespace"
            kubectl -n monitoring get svc -o wide --show-labels
          exit 1
          fi
          echo "Using Prometheus service: $SVC"

          # 4) Poll ~3 minutes for the alert (Prometheus needs to load rules + satisfy 'for: 1m')
          for i in {1..18}; do
            echo "Attempt $i/18..."
            if kubectl run curlmon --image=curlimages/curl:8.8.0 -n monitoring --restart=Never --rm -i -- \
              sh -lc "curl -sf http://${SVC}.monitoring.svc.cluster.local:9090/api/v1/alerts | grep -q SyntheticAlwaysFiring"
            then
              echo "SyntheticAlwaysFiring present."
              exit 0
            fi
            sleep 10
          done
          echo "Timed out waiting for SyntheticAlwaysFiring."
          exit 1


      - name: Docker login (GHCR)
        run: echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u "${{ github.actor }}" --password-stdin

      - name: Build & push image to GHCR (capture digest)
        run: |
          set -euo pipefail
          IMAGE="${REGISTRY}/${OWNER}/${IMAGE_NAME}:${TAG}"
          docker build -t "$IMAGE" .
          docker push "$IMAGE"
          FULL=$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE")
          echo "FULL=$FULL" >> $GITHUB_ENV
          echo "DIGEST=${FULL#*@}" >> $GITHUB_ENV
          echo "Published: $FULL"

      # If your GHCR package is private, allow pulls from default SA
      - name: Create imagePullSecret for GHCR (optional)
        run: |
          kubectl create secret docker-registry ghcr-pull \
            --docker-server=ghcr.io \
            --docker-username='${{ github.actor }}' \
            --docker-password='${{ secrets.GITHUB_TOKEN }}' \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -p '{"imagePullSecrets":[{"name":"ghcr-pull"}]}' || true

      - name: Install cert-manager (ephemeral cluster)
        shell: bash
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.yaml
          kubectl -n cert-manager rollout status deploy/cert-manager --timeout=180s
          kubectl -n cert-manager rollout status deploy/cert-manager-webhook --timeout=180s
          kubectl -n cert-manager rollout status deploy/cert-manager-cainjector --timeout=180s

      - name: Create/Update app Secret from repo secret
        run: |
          kubectl create secret generic myapp-secrets \
            --from-literal=SECRET_TOKEN='${{ secrets.SECRET_TOKEN }}' \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Apply DEV overlay pinned by digest (kustomize)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p .tmp/dev
          {
            echo "resources:"
            echo "  - ../../k8s/overlays/dev"
            echo "images:"
            echo "  - name: myapp"
            echo "    newName: ${REGISTRY}/${OWNER}/${IMAGE_NAME}"
            echo "    digest: ${DIGEST}"
          } > .tmp/dev/kustomization.yaml
          kubectl apply -k .tmp/dev
          kubectl rollout status deploy/myapp --timeout=180s

      - name: Debug on rollout failure
        if: failure()
        run: |
          kubectl get pods -l app=myapp -o wide
          kubectl describe pod -l app=myapp | sed -n '1,200p' || true
          kubectl logs -l app=myapp --all-containers --tail=200 || true
          kubectl get events --sort-by=.metadata.creationTimestamp | tail -n 80 || true


      - name: Wait for TLS cert (skip if you rely on ingress-shim)
        run: kubectl wait --for=condition=ready certificate/myapp-cert --timeout=180s || true

      - name: Verify image digest in running pod
        run: |
          POD=$(kubectl get pods -l app=myapp -o jsonpath='{.items[0].metadata.name}')
          IMG=$(kubectl get pod "$POD" -o jsonpath='{.spec.containers[0].image}')
          echo "Pod image: $IMG"  # expect ghcr.io/...@sha256:*

      - name: Verify Ingress TLS (in-cluster)
        run: |
          IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.clusterIP}')
          kubectl run curl --image=curlimages/curl:8.8.0 --restart=Never --rm -i -- \
            sh -lc 'curl -ks -H "Host: myapp.local" https://'"$IP"'/' || true

      - name: Ensure stage namespace
        if: ${{ github.ref == 'refs/heads/stage' }}
        run: kubectl create ns stage --dry-run=client -o yaml | kubectl apply -f -

      # Optional: deploy STAGE overlay when pushing to 'stage' branch
      - name: Apply STAGE overlay pinned by digest
        if: ${{ github.ref == 'refs/heads/stage' }}
        shell: bash
        run: |
          mkdir -p .tmp/stage
          {
            echo "resources:"
            echo "  - ../../k8s/overlays/stage"
            echo "images:"
            echo "  - name: myapp"
            echo "    newName: ${REGISTRY}/${OWNER}/${IMAGE_NAME}"
            echo "    digest: ${DIGEST}"
          } > .tmp/stage/kustomization.yaml
          kubectl apply -k .tmp/stage
          kubectl rollout status deploy/myapp --timeout=180s

